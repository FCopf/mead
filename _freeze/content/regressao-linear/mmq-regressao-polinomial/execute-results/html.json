{
  "hash": "f5c64ed65610dcb632afba38ef0eac41",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"MÃ©todo dos MÃ­nimos Quadrados na RegressÃ£o Polinomial\"\nsubtitle: \"ImplementaÃ§Ã£o em Python usando Ãlgebra Matricial\"\ndescription: \"Tutorial prÃ¡tico para implementar o mÃ©todo dos mÃ­nimos quadrados em Python para modelos polinomiais, aplicando os conceitos de Ã¡lgebra linear e estatÃ­stica bÃ¡sica.\"\nCategories: [\n          \"RegressÃ£o polinomial\",\n          \"MÃ©todo dos MÃ­nimos Quadrados\",\n          \"Ãlgebra Matricial\",\n          \"Python\"\n        ]\n\nimage: \"images/mmq-regressao-polinomial.png\"\nexecute:\n  echo: true\n  warning: false\n  include: true\n  message: false\n---\n\n\n\n\n## ğŸ“š IntroduÃ§Ã£o\n\n::: {.callout-tip title=\"Objetivos\"}\n\nNeste tutorial, vamos implementar o **MÃ©todo dos MÃ­nimos Quadrados (MMQ)** em Python para ajustar um modelo de **regressÃ£o polinomial** de segundo grau.\n\n**Objetivo**: Encontrar os coeficientes $\\beta_0$, $\\beta_1$ e $\\beta_2$ da equaÃ§Ã£o $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$ que melhor se ajustam aos nossos dados.\n\n:::\n\n## ğŸ› ï¸ Importando as Bibliotecas\n\nVamos comeÃ§ar importando as bibliotecas necessÃ¡rias:\n\n::: {#1474fe7e .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd           # Para manipulaÃ§Ã£o de dados\nimport matplotlib.pyplot as plt  # Para criaÃ§Ã£o e manipulaÃ§Ã£o grÃ¡fica\nimport seaborn as sns        # Para criaÃ§Ã£o e manipulaÃ§Ã£o grÃ¡fica\nimport numpy as np           # Para operaÃ§Ãµes matemÃ¡ticas e matriciais\n```\n:::\n\n\n## ğŸ“Š Inserindo os Dados\n\nVamos trabalhar com dados que apresentam uma relaÃ§Ã£o quadrÃ¡tica. Ao invÃ©s de digitarmos os dados diretamente $y$ e $x$ como listas, iremos ler os dados a partir de um arquivo que estÃ¡ disponÃ­vel no link [regressao_polinomial_exemplo](https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv). O arquivo esta no formato `.csv` em que cada coluna Ã© separada por uma vÃ­rgula, um tipo de formataÃ§Ã£o muito comum.\n\n::: {#4ac60275 .cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\n```\n:::\n\n\nUtilizando a funÃ§Ã£o `read_csv()` da bilbioteca [Pandas](https://pandas.pydata.org/), os dados foram importados no formato de **data frame**, basicamento uma estrutura de dados em linhas e colunas, em que as colunas sÃ£o denominadas de **atributos**.\n\n::: {#607aaadf .cell execution_count=3}\n``` {.python .cell-code}\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x   y\n0  0   5\n1  1   2\n2  2  10\n3  3   8\n4  4  15\n5  5  35\n```\n:::\n:::\n\n\n## ğŸ“ˆ Visualizando os Dados\n\nAntes de ajustar o modelo, vamos visualizar nossos dados:\n\n::: {#084ad5c7 .cell execution_count=4}\n``` {.python .cell-code}\n# Criando the grÃ¡fico de dispersÃ£o\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = df, x = 'x', y = 'y', color = '#0072B2', s=120, label='Dados observados')\n\n# Configurando o grÃ¡fico\nplt.title('GrÃ¡fico de DispersÃ£o dos Dados', fontsize=14, fontweight='bold')\nplt.xlabel('VariÃ¡vel X', fontsize=12)\nplt.ylabel('VariÃ¡vel Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq-regressao-polinomial_files/figure-html/cell-5-output-1.png){width=662 height=529}\n:::\n:::\n\n\n**ğŸ“ ObservaÃ§Ã£o 1**: Aparentemente, um modelo polinomial de segundo grau pode oferecer um ajuste melhor a estes dados do que a regressÃ£o linear simples. Nosso objetivo serÃ¡ explorar esse modelo e, ao final, comparÃ¡-lo com o modelo linear.\n\n**ğŸ“ ObservaÃ§Ã£o 2**: Como importamos os dados diretamente de um arquivo `.csv` para o objeto `df`, utilizamos a funÃ§Ã£o `scatterplot` da biblioteca [Seaborn](https://seaborn.pydata.org/) para plotar o grÃ¡fico de dispersÃ£o entre as variÃ¡veis $y$ e $x$.\n\n## ğŸ§® Implementando o MMQ Polinomial - Passo a Passo\n\n### Criando os Vetores Base\n\nPara o modelo polinomial $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$, precisamos dos vetores:\n\n$$\\vec{f}_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_1 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\vdots \\\\ x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#e2d2b877 .cell execution_count=5}\n``` {.python .cell-code}\n# NÃºmero de observaÃ§Ãµes\nn = len(df['x'])\n\n# Vetor f0: vetor de 1's (para o intercepto Î²â‚€)\nf0 = [1] * n\n\n# Vetor f1: valores de x (para o coeficiente linear Î²â‚)\nf1 = df['x'].copy()\n\n# Vetor f2: valores de xÂ² (para o coeficiente quadrÃ¡tico Î²â‚‚)\nf2 = np.array(df['x'])**2  # Eleva cada elemento de x ao quadrado\n```\n:::\n\n\nVisualizando os vetores $\\vec{f}_0$, $\\vec{f}_1$ e $\\vec{f}_2$.\n\n::: {#a64d9797 .cell execution_count=6}\n``` {.python .cell-code}\nprint(\"Vetor f0 (intercepto):\", f0)\nprint(\"Vetor f1 (termo linear):\", f1)\nprint(\"Vetor f2 (termo quadrÃ¡tico):\", f2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVetor f0 (intercepto): [1, 1, 1, 1, 1, 1]\nVetor f1 (termo linear): 0    0\n1    1\n2    2\n3    3\n4    4\n5    5\nName: x, dtype: int64\nVetor f2 (termo quadrÃ¡tico): [ 0  1  4  9 16 25]\n```\n:::\n:::\n\n\n### Construindo as Matrizes X e Y\n\nAgora vamos montar as matrizes do sistema polinomial:\n\n$$X = \\begin{bmatrix} \\vec{f}_0 & \\vec{f}_1 & \\vec{f}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad Y = \\begin{bmatrix} \\vec{y} \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#552fa520 .cell execution_count=7}\n``` {.python .cell-code}\n# Matriz X: combinando f0, f1 e f2 em colunas\nX = np.column_stack((f0, f1, f2))\n\n# Matriz Y: transformando y em matriz com n linhas e 1 coluna\nY = np.array(df['y']).reshape(n, 1)\n```\n:::\n\n\n### Resolvendo o Sistema Normal\n\nCalculamos os coeficientes usando a mesma fÃ³rmula: \n\n$$B = (X^T X)^{-1} X^T Y$$\n\n::: {#a69a7356 .cell execution_count=8}\n``` {.python .cell-code}\n# Calculando X transposta vezes X\nXTX = X.T @ X  # X.T Ã© a transposta de X\n# Calculando X transposta vezes Y\nXTY = X.T @ Y\n# Calculando a matriz inversa (X^T X)^(-1)\nXTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X\n# Coeficientes de regressÃ£o\nB = XTX_inv @ XTY\n```\n:::\n\n\n::: {.callout-note title=\"InterpretaÃ§Ã£o\"}\n\n- $\\beta_0$ (intercepto): valor de y quando x = 0\n- $\\beta_1$ (coeficiente linear): relacionado Ã  taxa de variaÃ§Ã£o linear\n- $\\beta_2$ (coeficiente quadrÃ¡tico): relacionado Ã  curvatura da parÃ¡bola\n  - Se $\\beta_2 > 0$: parÃ¡bola com concavidade para cima\n  - Se $\\beta_2 < 0$: parÃ¡bola com concavidade para baixo\n\n:::\n\n### Obtendo os Valores Ajustados de y\n\nTendo obtido os coeficientes de regressÃ£o, os valores ajustados de y ($\\hat{y}$) podem ser obtido pela multiplicaÃ§Ã£o matricial:\n\n$$F = XB = \\begin{bmatrix} 1 & x_1 & x^2_1 \\\\ 1 & x_2 & x^2_2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x^2_n \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}$$\n\n**Obs.**: denominamos $F$ a matriz de valores ajustados de $y$.\n\n::: {#298fd7b7 .cell execution_count=9}\n``` {.python .cell-code}\n# Valores ajustados (preditos)\nF = X @ B\n```\n:::\n\n\n### Avaliando a Qualidade do Ajuste\n\n#### Calculando a Soma dos Quadrados dos ResÃ­duos ($SQ_{res}$)\n\n$SQ_{res}$ pode ser obtida pela multiplicaÃ§Ã£o matricial:\n\n$$SQ_{res} = \\boldsymbol{e}^T \\boldsymbol{e}$$\n\nEm que $\\boldsymbol{e}$ Ã© a matriz coluna dos **resÃ­duos** obtida pela diferenÃ§a entre os valores observados e ajustados de $y$:\n\n$$\\boldsymbol{e} = Y - F = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}$$\n\n::: {#e65676ae .cell execution_count=10}\n``` {.python .cell-code}\n# ResÃ­duos: diferenÃ§a entre valores observados e ajustados\ne = Y - F\n\n# Soma dos Quadrados dos ResÃ­duos\nSQres = (e.T @ e)[0, 0]\n```\n:::\n\n\n#### Calculando a Soma dos Quadrados Totais ($SQ_{tot}$)\n\n$SQ_{tot}$ pode ser obtida pela multiplicaÃ§Ã£o matricial:\n\n$$SQ_{tot} = \\boldsymbol{D}^T \\boldsymbol{D}$$\n\nEm que $\\boldsymbol{D}$ Ã© a matriz coluna dos **desvios da mÃ©dis** obtida pela diferenÃ§a entre os valores observados de $y$ e a mÃ©dia de $\\overline{y}$:\n\n$$\\boldsymbol{D} = Y - \\overline{Y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\overline{y} \\\\ \\overline{y} \\\\ \\vdots \\\\ \\overline{y} \\end{bmatrix} = \\begin{bmatrix} d_1 \\\\ d_2 \\\\ \\vdots \\\\ d_n \\end{bmatrix}$$\n\n::: {#3c4e268a .cell execution_count=11}\n``` {.python .cell-code}\n# Soma dos Quadrados Total\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = (D.T @ D)[0, 0]\n```\n:::\n\n\n#### Calculando o coeficiente de determinaÃ§Ã£o $R^2$:\n\nO $R^2$ Ã© dado pela expressÃ£o:\n\n$$R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}$$\n\n::: {#dc67150f .cell execution_count=12}\n``` {.python .cell-code}\n# Coeficiente de DeterminaÃ§Ã£o RÂ²\nR2 = 1 - (SQres / SQtot)\n```\n:::\n\n\n---\n\nVisualizando os resultados:\n\n::: {#ce3b0804 .cell execution_count=13}\n``` {.python .cell-code}\nprint(\"ğŸ“Š Medidas de Qualidade do Ajuste:\")\nprint(f\"Soma dos Quadrados dos ResÃ­duos (SQres): {SQres:.4f}\")\nprint(f\"Soma dos Quadrados Total (SQtot): {SQtot:.4f}\")\nprint(f\"Coeficiente de DeterminaÃ§Ã£o (RÂ²): {R2:.4f}\")\nprint(f\"Porcentagem da variaÃ§Ã£o explicada: {R2*100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ“Š Medidas de Qualidade do Ajuste:\nSoma dos Quadrados dos ResÃ­duos (SQres): 59.2643\nSoma dos Quadrados Total (SQtot): 705.5000\nCoeficiente de DeterminaÃ§Ã£o (RÂ²): 0.9160\nPorcentagem da variaÃ§Ã£o explicada: 91.60%\n```\n:::\n:::\n\n\n## ğŸ“Š Visualizando o Resultado Final\n\nVamos plotar os dados originais junto com a curva ajustada:\n\nCriando uma linha `contÃ­nua` para $\\hat{y}$\n\n::: {#3f7c4870 .cell execution_count=14}\n``` {.python .cell-code}\n# Criando pontos para desenhar a curva suave\nx_curva = np.linspace(min(df['x']), max(df['x']), 100)\ny_curva = B[0, 0] + B[1, 0] * x_curva + B[2, 0] * x_curva**2\n```\n:::\n\n\n::: {#054ef012 .cell execution_count=15}\n``` {.python .cell-code}\n# Criando o grÃ¡fico final\nplt.figure(figsize=(8, 6))\n\n# Pontos observados\nsns.scatterplot(data = df, x = 'x', y = 'y', \n                color = '#0072B2', s=120,\n                label=f'Dados observados (n={n})')\n\n# Valores ajustados\nplt.scatter(df['x'], F[:,0], \n           color='#000000', marker='*', s=120, \n           label='Valores ajustados')\n\n# Curva ajustada\nplt.plot(x_curva, y_curva, \n         color='#D55E00', \n         label=fr'Curva ajustada: $\\hat{{y}} = {B[0,0]:.3f} {B[1,0]:.3f}x + {B[2,0]:.3f}x^2$')\n\n# ConfiguraÃ§Ãµes do grÃ¡fico\nplt.title(f'RegressÃ£o Polinomial (2Âº grau) - MMQ\\nRÂ² = {R2:.4f}', \n          fontsize=15, fontweight='bold')\nplt.xlabel('VariÃ¡vel X', fontsize=12)\nplt.ylabel('VariÃ¡vel Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq-regressao-polinomial_files/figure-html/cell-16-output-1.png){width=759 height=569}\n:::\n:::\n\n\n## ğŸ¯ Resumo dos Resultados\n\n::: {#dd1692cd .cell execution_count=16}\n``` {.python .cell-code}\nprint(\"=\"*60)\nprint(\"         RESUMO DA REGRESSÃƒO POLINOMIAL\")\nprint(\"=\"*60)\nprint(f\"EquaÃ§Ã£o ajustada: y = {B[0,0]:.4f} {B[1,0]:.4f}x + {B[2,0]:.4f}xÂ²\")\nprint(f\"Coeficiente de determinaÃ§Ã£o (RÂ²): {R2:.4f}\")\nprint(f\"Porcentagem da variaÃ§Ã£o explicada: {R2*100:.2f}%\")\nprint(\"=\"*60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n============================================================\n         RESUMO DA REGRESSÃƒO POLINOMIAL\n============================================================\nEquaÃ§Ã£o ajustada: y = 5.7500 -4.5679x + 1.9821xÂ²\nCoeficiente de determinaÃ§Ã£o (RÂ²): 0.9160\nPorcentagem da variaÃ§Ã£o explicada: 91.60%\n============================================================\n```\n:::\n:::\n\n\n## ğŸ” ComparaÃ§Ã£o: Linear vs Polinomial\n\nVamos comparar o ajuste linear e polinomial para os mesmos dados:\n\n::: {#fe75c88f .cell execution_count=17}\n``` {.python .cell-code}\n# Ajuste LINEAR para comparaÃ§Ã£o\nX_linear = np.column_stack((f0, f1))  # Apenas f0 e f1\nB_linear = np.linalg.inv(X_linear.T @ X_linear) @ (X_linear.T @ Y)\n\n# RÂ² do modelo linear\nF_linear = X_linear @ B_linear\nresiduos_linear = Y - F_linear\nSQres_linear = (residuos_linear.T @ residuos_linear)[0, 0]\nR2_linear = 1 - (SQres_linear / SQtot)\n\nprint(\"ğŸ“Š ComparaÃ§Ã£o dos Modelos:\")\nprint(\"-\" * 40)\nprint(f\"Modelo Linear:     RÂ² = {R2_linear:.4f}\")\nprint(f\"Modelo Polinomial: RÂ² = {R2:.4f}\")\nprint(f\"Melhoria no RÂ²:    {R2 - R2_linear:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ“Š ComparaÃ§Ã£o dos Modelos:\n----------------------------------------\nModelo Linear:     RÂ² = 0.7081\nModelo Polinomial: RÂ² = 0.9160\nMelhoria no RÂ²:    0.2079\n```\n:::\n:::\n\n\nGrÃ¡ficos de dispersÃ£o\n\n::: {#5f7ca3aa .cell execution_count=18}\n``` {.python .cell-code}\nx_vals = df[\"x\"].to_numpy()\n\ny_linear = B_linear[0, 0] + B_linear[1, 0] * x_vals\n\n# GrÃ¡fico comparativo\nplt.figure(figsize=(8, 6))\n\n# plt.subplot(1, 2, 1)\nsns.scatterplot(data = df, x = 'x', y = 'y', s=100, color = '#0072B2', label='Dados observados')\nplt.plot(x_vals, y_linear, color='#D55E00', label=f'Modelo Linear\\nRÂ² = {R2_linear:.4f}')\nplt.plot(x_curva, y_curva, color='#009E73', label=f'Modelo Polinomial\\nRÂ² = {R2:.4f}')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# plt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq-regressao-polinomial_files/figure-html/cell-19-output-1.png){width=659 height=503}\n:::\n:::\n\n\n## ğŸ§¾ Resumo do CÃ³digo (modelo polinomial)\n\n1. InserÃ§Ã£o dos Dados\n\n::: {#40bf6862 .cell execution_count=19}\n``` {.python .cell-code}\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\n```\n:::\n\n\n2. DefiniÃ§Ã£o das matrizes do sistema\n\n::: {#ffaa98de .cell execution_count=20}\n``` {.python .cell-code}\nn = len(df['x'])\nf0 = [1] * n\nf1 = df['x'].copy()\nf2 = np.array(df['x'])**2\n\nX = np.column_stack((f0, f1, f2))\nY = np.array(df['y']).reshape(n, 1)\n```\n:::\n\n\n3. CÃ¡lculo dos coeficientes\n\n::: {#dc726319 .cell execution_count=21}\n``` {.python .cell-code}\nXTX = X.T @ X\nXTY = X.T @ Y\nXTX_inv = np.linalg.inv(XTX)\nB = XTX_inv @ XTY\n```\n:::\n\n\n4. Qualidade do ajuste\n\n::: {#2fb32847 .cell execution_count=22}\n``` {.python .cell-code}\nY_ajustado = X @ B\ne = Y - Y_ajustado\nSQres = (e.T @ e)[0, 0]\n\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = (D.T @ D)[0, 0]\n\nR2 = 1 - (SQres / SQtot)\n```\n:::\n\n\n## ğŸš€ ExercÃ­cio PrÃ¡tico\n\nTeste o cÃ³digo com novos dados:\n\n::: {#4cc71cad .cell execution_count=23}\n``` {.python .cell-code}\n# Experimente com estes dados (padrÃ£o quadrÃ¡tico diferente):\ndf_novo = pd.DataFrame({\n  'x_novo': [1, 2, 3, 4, 5, 6, 7],\n  'y_novo': [30, 12, 18, 9, 7, 8, 6]\n})\n\nprint(df_novo)\n\n# QuestÃµes para investigar:\n# 1. Qual Ã© o RÂ² do modelo polinomial para estes dados?\n# 2. O coeficiente Î²â‚‚ Ã© positivo ou negativo? O que isso significa?\n# 3. Compare com o modelo linear - qual Ã© a diferenÃ§a no RÂ²?\n\n# Implemente todo o processo do MMQ polinomial com os novos dados\n# Dica: vocÃª pode copiar e adaptar o cÃ³digo acima!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x_novo  y_novo\n0       1      30\n1       2      12\n2       3      18\n3       4       9\n4       5       7\n5       6       8\n6       7       6\n```\n:::\n:::\n\n\n## ğŸ’¡ Conceitos Importantes Revisados\n\n1. **RegressÃ£o Polinomial**: ExtensÃ£o da regressÃ£o linear para relaÃ§Ãµes curvas\n2. **Matriz de Design**: Agora com trÃªs colunas: $[1, x, x^2]$\n3. **InterpretaÃ§Ã£o dos Coeficientes**: Cada coeficiente tem significado especÃ­fico\n4. **ComparaÃ§Ã£o de Modelos**: Uso do $R^2$ para avaliar qual modelo Ã© melhor\n\n## ğŸ”— PrÃ³ximos Passos\n\n- Experimente com polinÃ´mios de grau maior ($x^3$, $x^4$, etc.)\n- Investigue o conceito de **overfitting** com graus muito altos\n\n",
    "supporting": [
      "mmq-regressao-polinomial_files"
    ],
    "filters": [],
    "includes": {}
  }
}