{
  "hash": "bf7d9f2854bc7b555831addc32f27d78",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Explorando Modelos de Regressão Bayesiana\"\nsubtitle: \"Dos modelos lineares a respostas generalizadas e estruturas hierárquicas\"\nauthor: \"Prof. Fabio Cop\"\nimage: \"images/regressao-glm-hierarquico-apresentacao.png\"\nformat:\n  revealjs: \n    institute: Laboratório de Ecologia e Dinâmica de Comunidades (LaEDCom)\n    slide-number: true\n    chalkboard: \n      buttons: false\n    preview-links: auto\n    logo: images/logo-white.png \n    footer: |\n      Instituto do Mar - Unifesp · <a href=\"https://fcopf.github.io/MEAD/\" target=\"_blank\">Métodos em Estatística e Análise de Dados</a>\n    theme: [default, custom.scss]\ntitle-slide-attributes:\n   data-background: \"linear-gradient(135deg, #d0dee4, #b8c0ce, #d6e0e4)\"\n\nexecute:\n    eval: true\n---\n\n\n\n\n## O que aprendemos até aqui: prioris e posterioris\n\n<iframe src=\"https://fcopf-binomial-bayesiana.share.connect.posit.cloud/\" style=\"width: 100%; height: 80%; zoom: 0.6;\" frameborder=\"0\"></iframe>\n\n---\n\n## O que aprendemos até aqui: distribuição Normal de Probabilidade\n\n$$\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2} \\left(\\frac{y - \\mu}{\\sigma} \\right)^2} \\longrightarrow \\quad y \\sim \\mathcal{N}(\\mu, \\sigma)\n$$\n\n![](images/normal-distributions.png){width=100% fig-align=\"center\"}\n\n\n---\n\n## O que aprendemos até aqui: o modelo de Regressão linear\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n\n---\n\n**Variável aleatória resposta**\n\n---\n\n$$\ny \\sim \\mathcal{N}(\\mu, \\sigma)\n$$\n\n$$\n\\mu = \\beta_0 + \\beta_1 x\n$$\n\n---\n\n**Prioris**\n\n---\n\n$$\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n$$\n\n$$\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1})\n$$\n\n$$\n\\sigma \\sim \\text{Lognormal}(\\mu_{\\log \\sigma}, \\sigma_{\\log \\sigma})\n$$\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](images/Regre2.png){width=150%}\n\n:::\n\n:::\n\n---\n\n## O que aprendemos até aqui: Programação Probabilística\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n\n---\n\n**PyMC**\n\n---\n\n```{.python code-line-numbers=\"|1|2-5|7-10|12-13\"}\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=60, sigma=5)\n    calcado = pm.Normal(\"calcado\", mu=2.8, sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n    \n    # Definição do modelo\n    mu = beta_0 + beta_1 * X\n    altura = pm.Normal(\"altura\", mu=mu, sigma=sigma, \n                        observed=Y)\n    \n    # Amostra a distribuição posterior\n    resultados = pm.sample()\n```\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n---\n\n**Bambi**\n\n---\n\n```{.python code-line-numbers=\"|1-6|8-10|12-13\"}\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=60, sigma=5),\n    \"calcado\": bmb.Prior(\"Normal\", mu=2.8, sigma=0.1),\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=10)\n}\n\n# Definição do modelo\nmodelo = bmb.Model(\"altura ~ calcado\", df, \n                    priors=custom_priors)\n\n# Amostra a distribuição posterior\nresultados = modelo.fit()\n```\n\n:::\n\n:::\n\n---\n\n## O que aprendemos até aqui: ajuste da posteriori\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n![](images/trace-plots.png)\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](images/regressao-linear-retas.png)\n\n:::\n\n:::\n\n---\n\n## Daqui para frente: uma variedade de modelos e estruturas\n\n1.  Extenção da Regressão Linear para:\n    -   Múltiplos preditores.\n    -   Diferentes tipos de variáveis resposta (GLMs).\n    -   Dados com estrutura de agrupamento (Modelos Hierárquicos).\n\n\n---\n\n## Regressão linear múltipla\n\n\n::: columns\n\n\n::: {.column width=\"50%\"}\n\n---\n\n**Variável aleatória resposta**\n\n---\n\n$$\ny \\sim \\mathcal{N}(\\mu, \\sigma)\n$$\n\n$$\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k\n$$\n\n---\n\n**Prioris**\n\n---\n\n$$\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n$$\n\n$$\n\\beta_j \\sim \\mathcal{N}(\\mu_{\\beta_j}, \\sigma_{\\beta_j}) \\quad \\text{para } j = 1, \\dots, k\n$$\n\n$$\n\\sigma \\sim \\text{Lognormal}(\\mu_{\\log \\sigma}, \\sigma_{\\log \\sigma})\n$$\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n<!-- ![](images/RegreMultipla.png){width=150%} -->\n\n::: {#3c790861 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](regressao-glm-hierarquico-apresentacao_files/figure-revealjs/cell-2-output-1.png){width=548 height=537}\n:::\n:::\n\n\n:::\n\n\n:::\n\n---\n\n## Programação Probabilística\n\n\n::: columns\n\n\n::: {.column width=\"50%\"}\n\n---\n\n**PyMC**\n\n---\n\n```{.python}\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=60, sigma=5)\n    beta_1 = pm.Normal(\"beta_1\", mu=2.8, sigma=0.1)\n    beta_2 = pm.Normal(\"beta_2\", mu=1.5, sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n\n    # Definição do modelo\n    mu = Intercept + beta_1 * X1 + beta_2 * X2\n    altura = pm.Normal(\"altura\", mu=mu, sigma=sigma, \n                       observed=Y)\n\n    # Amostra a distribuição posterior\n    resultados = pm.sample()\n```\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n---\n\n**Bambi**\n\n---\n\n```{.python}\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=60, sigma=5),\n    \"X1\": bmb.Prior(\"Normal\", mu=2.8, sigma=0.1),\n    \"X2\": bmb.Prior(\"Normal\", mu=1.5, sigma=0.1),\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=10)\n}\n\n# Definição do modelo\nmodelo = bmb.Model(\"altura ~ X1 + X2\", df, \n                   priors=custom_priors)\n\n# Amostra a distribuição posterior\nresultados = modelo.fit()\n\n```\n\n\n:::\n\n\n:::\n\n---\n\n## Regressão de Poisson: dados de contagem\n\n::: columns\n\n::: {.column width=\"30%\"}\n$$\ny \\sim \\text{Poisson}(\\lambda)\n$$\n\n$$\n\\log(\\lambda) = \\mu = \\beta_0 + \\beta_1 x\n$$\n\n:::\n\n::: {.column width=\"70%\"}\n\n::: {#7bc3e1f5 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](regressao-glm-hierarquico-apresentacao_files/figure-revealjs/cell-3-output-1.png){width=950 height=566}\n:::\n:::\n\n\n::: \n\n::: \n\n---\n\n## Regressão de Poisson: dados de contagem\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n$$\nf(y) = \\frac{e^{-\\lambda} \\lambda^y}{y!} \\longrightarrow \\quad y \\sim \\text{Poisson}(\\lambda)\n$$\n\n$$\n\\log(\\lambda) = \\mu\n$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n---\n\n**Variável aleatória resposta**\n\n---\n\n$$\ny \\sim \\text{Poisson}(\\lambda)\n$$\n\n$$\n\\log(\\lambda) = \\mu = \\beta_0 + \\beta_1 x\n$$\n\n---\n\n**Prioris**\n\n---\n\n$$\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n$$\n\n$$\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1})\n$$\n\n:::\n\n:::\n\n---\n\n## Programação Probabilística\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n---\n\n**PyMC**\n\n---\n\n```{.python}\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=0, sigma=5)\n    beta = pm.Normal(\"beta\", mu=0, sigma=2)\n    \n    # Função de ligação log: log(λ) = μ = Intercept + beta * X\n    mu = Intercept + beta * X\n    lambda_ = pm.math.exp(mu)\n    \n    # Modelo de verossimilhança\n    contagem = pm.Poisson(\"contagem\", mu=lambda_, \n                          observed=Y)\n    \n    # Amostragem\n    resultados = pm.sample()\n```\n\n:::\n\n::: {.column width=\"50%\"}\n\n---\n\n**Bambi**\n\n---\n\n```{.python}\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=0, sigma=5),\n    \"x\": bmb.Prior(\"Normal\", mu=0, sigma=2),\n}\n\n# Modelo com função de ligação log (default da família Poisson)\nmodelo = bmb.Model(\"contagem ~ x\", df, \n                   family=\"poisson\", priors=custom_priors)\n\n# Amostragem\nresultados = modelo.fit()\n```\n\n:::\n\n:::\n\n---\n\n## Regressão Logística: dados dicotômicos\n\n::: columns\n\n::: {.column width=\"40%\"}\n\n$$\ny \\sim \\text{Bernoulli}(p)\n$$\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right) = \\mu = \\beta_0 + \\beta_1 x\n$$\n\n$$\np = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\n$$\n\n:::\n\n::: {.column width=\"60%\"}\n\n::: {#948ebc88 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](regressao-glm-hierarquico-apresentacao_files/figure-revealjs/cell-4-output-1.png){width=950 height=565}\n:::\n:::\n\n\n:::\n\n:::\n\n---\n\n## Regressão Logística: dados dicotômicos\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n---\n\n**Distribuição da variável resposta**\n\n---\n\n$$\nf(y) = p^y (1 - p)^{1 - y}\n$$\n\n$$\ny \\sim \\text{Bernoulli}(p)\n$$\n\n---\n\n**Função de ligação**\n\n---\n\n$$\n\\text{logit}(p) = \\mu = \\beta_0 + \\beta_1 x\n$$\n\n$$\n\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n$$\n\n$$\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x\n$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n$$\n\\frac{p}{1 - p} = e^{\\beta_0 + \\beta_1 x}\n$$\n\n\n$$\np = (1 - p) \\cdot e^{\\beta_0 + \\beta_1 x}\n$$\n\n\n$$\np = e^{\\beta_0 + \\beta_1 x} - p \\cdot e^{\\beta_0 + \\beta_1 x}\n$$\n\n$$\np \\left(1 + e^{\\beta_0 + \\beta_1 x}\\right) = e^{\\beta_0 + \\beta_1 x}\n$$\n\n$$\np = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n$$\n\n:::\n\n:::\n\n---\n\n## Programação Probabilística\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n---\n\n**PyMC**\n\n---\n\n```{.python}\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=0, sigma=5)\n    beta = pm.Normal(\"beta\", mu=0, sigma=2)\n    \n    # Preditor linear e função de ligação logit\n    mu = Intercept + beta * X\n    p = pm.math.sigmoid(mu)\n    \n    # Verossimilhança\n    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=Y)\n    \n    # Amostragem\n    resultados = pm.sample()\n```\n\n:::\n\n::: {.column width=\"50%\"}\n\n---\n\n**Bambi**\n\n---\n\n```{.python}\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=0, sigma=5),\n    \"x\": bmb.Prior(\"Normal\", mu=0, sigma=2),\n}\n\n# Modelo logístico (ligação logit é padrão para bernoulli)\nmodelo = bmb.Model(\"y ~ x\", df, \n                   family=\"bernoulli\", priors=custom_priors)\n\n# Amostragem\nresultados = modelo.fit()\n```\n\n:::\n\n:::\n\n---\n\n## Modelo Hierárquico Normal com Intercepto e Inclinação Variáveis\n\n::: columns\n\n::: {.column width=\"30%\"}\n\n$$\ny_{ij} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma^2)\n$$\n\n$$\n\\mu_{ij} = \\beta_{0j} + \\beta_{1j} x_{ij}\n$$\n\n$$\n\\beta_{0j} \\sim \\mathcal{N}(\\gamma_0, \\tau_0^2) \\\\\n\\beta_{1j} \\sim \\mathcal{N}(\\gamma_1, \\tau_1^2)\n$$\n\n:::\n\n::: {.column width=\"70%\"}\n\n::: {#818bcb27 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](regressao-glm-hierarquico-apresentacao_files/figure-revealjs/cell-5-output-1.png){width=950 height=565}\n:::\n:::\n\n\n:::\n\n:::\n\n---\n\n## Variação entre grupos: coeficientes do modelo hierárquico\n\n::: columns\n\n::: {.column width=\"30%\"}\n\nCoeficientes específicos por grupo:\n\n$$\n\\beta_{0j} \\sim \\mathcal{N}(\\gamma_0, \\tau_0^2) \\\\\n\\beta_{1j} \\sim \\mathcal{N}(\\gamma_1, \\tau_1^2)\n$$\n\nVisualizando a dispersão dos parâmetros em relação às médias populacionais $\\gamma_0, \\gamma_1$.\n\n:::\n\n::: {.column width=\"70%\"}\n\n::: {#d7f4ea4d .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](regressao-glm-hierarquico-apresentacao_files/figure-revealjs/cell-6-output-1.png){width=755 height=563}\n:::\n:::\n\n\n:::\n\n:::\n\n",
    "supporting": [
      "regressao-glm-hierarquico-apresentacao_files"
    ],
    "filters": [],
    "includes": {}
  }
}