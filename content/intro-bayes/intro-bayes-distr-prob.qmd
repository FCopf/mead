---
title: "De contagens a probabilidades"  
subtitle: "A distribuiÃ§Ã£o *a posteriori*"  
description: "TransiÃ§Ã£o de contagens para probabilidades sob uma perspectiva bayesiana. Baseado em *Statistical Rethinking* [@mcelreath2018statistical]."  
Categories: [
          "InferÃªncia bayesiana",
          "DistribuiÃ§Ã£o de probabilidade",
          "DistribuiÃ§Ã£o a priori",
          "DistribuiÃ§Ã£o posterior"
        ]

image: "images/intro-bayes-distr-prob.png"  
execute:  
  echo: false  
  warning: false  
  include: true  
  message: false  

---

---

Voltemos ao problema das bolinhas de gude. Temos uma caixa contendo quatro bolinhas, que podem ser azuis ou brancas. Sabemos que hÃ¡ exatamente quatro bolinhas, mas nÃ£o conhecemos a distribuiÃ§Ã£o entre as cores, pois podemos ver apenas uma bolinha por vez atravÃ©s de um orifÃ­cio. Para estimar quantas bolas de cada cor hÃ¡ na caixa, fazemos uma observaÃ§Ã£o, misturamos as bolinhas, fazemos outra observaÃ§Ã£o e assim por diante. Antes de realizarmos qualquer observaÃ§Ã£o, podemos listar cinco configuraÃ§Ãµes possÃ­veis para o conteÃºdo da caixa:

1. [âšªâšªâšªâšª]  
2. [ğŸ”µâšªâšªâšª]  
3. [ğŸ”µğŸ”µâšªâšª]  
4. [ğŸ”µğŸ”µğŸ”µâšª]  
5. [ğŸ”µğŸ”µğŸ”µğŸ”µ]

Nosso objetivo Ã© *estimar o nÃºmero $N$ de bolas azuis*, o qual pode variar, neste exemplo, de 0 a 4. Como nÃ£o dispomos de conhecimento prÃ©vio sobre a composiÃ§Ã£o da caixa antes da primeira observaÃ§Ã£o, adotamos uma distribuiÃ§Ã£o *a priori* uniforme entre as hipÃ³teses. Assim, cada hipÃ³tese recebe uma probabilidade de $p = \frac{1}{5}$.

| **HipÃ³tese** | **N** | **Priori** |
|:------------:|:-----:|:----------:|
| [âšªâšªâšªâšª]   |  0    | $1/5$    |
| [ğŸ”µâšªâšªâšª]   |  1    | $1/5$    |
| [ğŸ”µğŸ”µâšªâšª]   |  2    | $1/5$    |
| [ğŸ”µğŸ”µğŸ”µâšª]   |  3    | $1/5$    |
| [ğŸ”µğŸ”µğŸ”µğŸ”µ]   |  4    | $1/5$    |

: **DistribuiÃ§Ã£o de probabilidade *a priori* sobre o conteÃºdo da caixa** {#tbl-priori} 

---

## DistribuiÃ§Ãµes *a priori*, *a posteriori* e *verossimilhanÃ§a*

Suponha que realizamos trÃªs observaÃ§Ãµes da caixa e a sequÃªncia registrada seja [ğŸ”µ, âšª, ğŸ”µ] â€“ ou seja, duas bolas azuis. Para atualizar nosso conhecimento sobre a composiÃ§Ã£o da caixa, combinamos nossa distribuiÃ§Ã£o *a priori* com a verossimilhanÃ§a de cada hipÃ³tese. A verossimilhanÃ§a Ã© calculada a partir da contagem do nÃºmero de maneiras em que cada hipÃ³tese pode gerar a sequÃªncia observada. Em seguida, aplicamos a regra de Bayes, que nos fornece a distribuiÃ§Ã£o *a posteriori* por meio da fÃ³rmula:

$$\text{Posterior}_i = \frac{\text{Priori}_i \times P_i}{\sum_{j} \left(\text{Priori}_j \times P_j\right)},$$ {#eq-bayes}

onde $P_i$ representa, de forma proporcional, o nÃºmero de caminhos possÃ­veis para que a hipÃ³tese $i$ gere a sequÃªncia [ğŸ”µ, âšª, ğŸ”µ].

A equaÃ§Ã£o @eq-bayes indica que, para cada valor que $N$ pode assumir, julgamos sua plausibilidade como proporcional ao nÃºmero de maneiras pelas quais esse valor pode ter gerado os dados, multiplicado pela sua evidÃªncia anterior (a distribuiÃ§Ã£o *a priori*). Esse produto â€“ representado na @tbl-posteriori por $\text{Priori}_i \times P_i$ â€“ Ã© entÃ£o *normalizado*, dividindo cada um deles pelo somatÃ³rio $\sum (1/5 \times \text{NÂº de caminhos})$. Essa normalizaÃ§Ã£o gera a distribuiÃ§Ã£o **a posteriori**, que reflete nosso conhecimento atualizado apÃ³s a incorporaÃ§Ã£o das evidÃªncias observadas.


Na @tbl-posteriori, a coluna *â€œManeiras de produzir N = 2 [ğŸ”µâšªğŸ”µ]â€* indica o nÃºmero de caminhos possÃ­veis para que cada hipÃ³tese gere a sequÃªncia observada. Note que as hipÃ³teses [âšªâšªâšªâšª] e [ğŸ”µğŸ”µğŸ”µğŸ”µ] nÃ£o conseguem gerar a sequÃªncia (ou seja, possuem verossimilhanÃ§a zero). 

| **HipÃ³tese** | **N** | **Priori** | **Maneiras de produzir N = 2 [ğŸ”µâšªğŸ”µ]** | **Posterior** |
|:------------:|:-----:|:----------:|:---------------------------------------:|:-------------:|
| [âšªâšªâšªâšª]   |  0    | $1/5$    | $0 \times 4 \times 0 = 0$             | $\dfrac{(1/5 \times 0)}{\sum (1/5 \times \text{NÂº de caminhos})} = 0$  |
| [ğŸ”µâšªâšªâšª]   |  1    | $1/5$    | $1 \times 3 \times 1 = 3$             | $\dfrac{(1/5 \times 3)}{\sum (1/5 \times \text{NÂº de caminhos})} = 0.15$  |
| [ğŸ”µğŸ”µâšªâšª]   |  2    | $1/5$    | $2 \times 2 \times 2 = 8$             | $\dfrac{(1/5 \times 8)}{\sum (1/5 \times \text{NÂº de caminhos})} = 0.40$  |
| [ğŸ”µğŸ”µğŸ”µâšª]   |  3    | $1/5$    | $3 \times 1 \times 3 = 9$             | $\dfrac{(1/5 \times 9)}{\sum (1/5 \times \text{NÂº de caminhos})} = 0.45$  |
| [ğŸ”µğŸ”µğŸ”µğŸ”µ]   |  4    | $1/5$    | $4 \times 0 \times 4 = 0$             | $\dfrac{(1/5 \times 0)}{\sum (1/5 \times \text{NÂº de caminhos})} = 0$  |

: **AtualizaÃ§Ã£o da distribuiÃ§Ã£o de probabilidade combinando a priori e verossimilhanÃ§a** {#tbl-posteriori} 

Dessa forma, a plausibilidade de cada hipÃ³tese Ã© convertida em **probabilidades** â€“ valores nÃ£o negativos cuja soma Ã© igual a 1 â€“ para cada uma das hipÃ³teses sobre o conteÃºdo da caixa. O resultado final da inferÃªncia bayesiana Ã© fornecer uma base probabilÃ­stica para a tomada de decisÃ£o sobre um fenÃ´meno parcialmente desconhecido, expressando o quÃ£o plausÃ­vel Ã© cada hipÃ³tese Ã  luz dos dados disponÃ­veis (@fig-priori-posteriori).


```{python}
#| label: fig-priori-posteriori
#| fig-cap: "DistribuiÃ§Ã£o a priori (esquerda) e a posteriori (direita) para o nÃºmero de bolas azuis. A priori, as hipÃ³teses tÃªm probabilidade uniforme; a posteriori, a observaÃ§Ã£o [ğŸ”µ, âšª, ğŸ”µ] atualiza a plausibilidade, favorecendo hipÃ³teses intermediÃ¡rias"
import matplotlib.pyplot as plt
import numpy as np

# NÃºmero de bolas azuis possÃ­veis (0 a 4)
N = np.array([0, 1, 2, 3, 4])

# DistribuiÃ§Ã£o a priori uniforme
prior = np.full_like(N, 1/5, dtype=float)

# NÃºmero de caminhos (verossimilhanÃ§a proporcional) para [ğŸ”µ, âšª, ğŸ”µ]
likelihood = np.array([0, 3, 8, 9, 0])

# Produto da priori pela verossimilhanÃ§a proporcional
unnormalized_posterior = prior * likelihood

# NormalizaÃ§Ã£o
posterior = unnormalized_posterior / unnormalized_posterior.sum()

# Plot
fig, axs = plt.subplots(1, 2, figsize=(2.5*3, 3), sharey=True)

# GrÃ¡fico da distribuiÃ§Ã£o a priori
axs[0].bar(N, prior)
axs[0].set_title("DistribuiÃ§Ã£o a priori")
axs[0].set_xlabel("NÃºmero de bolas azuis (N)")
axs[0].set_ylabel("Probabilidade")
axs[0].set_xticks(N)

# GrÃ¡fico da distribuiÃ§Ã£o a posteriori
axs[1].bar(N, posterior)
axs[1].set_title("DistribuiÃ§Ã£o a posteriori")
axs[1].set_xlabel("NÃºmero de bolas azuis (N)")
axs[1].set_xticks(N)

plt.tight_layout()
plt.show()

```


Esses conceitos possuem nomenclaturas especÃ­ficas, e vale a pena aprendÃª-los, pois vocÃª os encontrarÃ¡ repetidamente:

- A conjectura sobre o nÃºmero de bolinhas azuis $N$ Ã© chamada de **valor do parÃ¢metro** â€“ uma maneira de indexar as possÃ­veis explicaÃ§Ãµes para os dados.
- O nÃºmero relativo de maneiras pelo qual esse parÃ¢metro pode produzir os dados Ã© chamado de **verossimilhanÃ§a** (*likelihood*). Essa medida Ã© obtida ao enumerar todas as sequÃªncias de dados possÃ­veis e, em seguida, descartar aquelas que nÃ£o sÃ£o compatÃ­veis com os dados observados.
- A plausibilidade anterior de um valor especÃ­fico Ã© denominada **distribuiÃ§Ã£o de probabilidade a priori**.
- A plausibilidade atualizada de um valor especÃ­fico, apÃ³s a incorporaÃ§Ã£o dos dados, Ã© denominada **distribuiÃ§Ã£o de probabilidade a posteriori**, utilizada para inferir a probabilidade de cada hipÃ³tese ou conjunto de hipÃ³teses sobre o parÃ¢metro.