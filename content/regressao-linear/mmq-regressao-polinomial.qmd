---
title: "MÃ©todo dos MÃ­nimos Quadrados na RegressÃ£o Polinomial"
subtitle: "ImplementaÃ§Ã£o em Python usando Ãlgebra Matricial"
description: "Tutorial prÃ¡tico para implementar o mÃ©todo dos mÃ­nimos quadrados em Python para modelos polinomiais, aplicando os conceitos de Ã¡lgebra linear e estatÃ­stica bÃ¡sica."
Categories: [
          "RegressÃ£o polinomial",
          "MÃ©todo dos MÃ­nimos Quadrados",
          "Ãlgebra Matricial",
          "Python"
        ]

image: "images/mmq-regressao-polinomial.png"
execute:
  echo: true
  warning: false
  include: true
  message: false
---

## ğŸ“š IntroduÃ§Ã£o

::: {.callout-tip title="Objetivos"}

Neste tutorial, vamos implementar o **MÃ©todo dos MÃ­nimos Quadrados (MMQ)** em Python para ajustar um modelo de **regressÃ£o polinomial** de segundo grau.

**Objetivo**: Encontrar os coeficientes $\beta_0$, $\beta_1$ e $\beta_2$ da equaÃ§Ã£o $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$ que melhor se ajustam aos nossos dados.

:::

## ğŸ› ï¸ Importando as Bibliotecas

Vamos comeÃ§ar importando as bibliotecas necessÃ¡rias:

```{python}
import pandas as pd           # Para manipulaÃ§Ã£o de dados
import matplotlib.pyplot as plt  # Para criaÃ§Ã£o e manipulaÃ§Ã£o grÃ¡fica
import seaborn as sns        # Para criaÃ§Ã£o e manipulaÃ§Ã£o grÃ¡fica
import numpy as np           # Para operaÃ§Ãµes matemÃ¡ticas e matriciais
```

## ğŸ“Š Inserindo os Dados

Vamos trabalhar com dados que apresentam uma relaÃ§Ã£o quadrÃ¡tica. Ao invÃ©s de digitarmos os dados diretamente $y$ e $x$ como listas, iremos ler os dados a partir de um arquivo que estÃ¡ disponÃ­vel no link [regressao_polinomial_exemplo](https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv). O arquivo esta no formato `.csv` em que cada coluna Ã© separada por uma vÃ­rgula, um tipo de formataÃ§Ã£o muito comum.

```{python}
df = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')
```

Utilizando a funÃ§Ã£o `read_csv()` da bilbioteca [Pandas](https://pandas.pydata.org/), os dados foram importados no formato de **data frame**, basicamento uma estrutura de dados em linhas e colunas, em que as colunas sÃ£o denominadas de **atributos**.

```{python}
print(df)
```

## ğŸ“ˆ Visualizando os Dados

Antes de ajustar o modelo, vamos visualizar nossos dados:

```{python}
# Criando the grÃ¡fico de dispersÃ£o
plt.figure(figsize=(8, 6))
sns.scatterplot(data = df, x = 'x', y = 'y', color = '#0072B2', s=120, label='Dados observados')

# Configurando o grÃ¡fico
plt.title('GrÃ¡fico de DispersÃ£o dos Dados', fontsize=14, fontweight='bold')
plt.xlabel('VariÃ¡vel X', fontsize=12)
plt.ylabel('VariÃ¡vel Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
```

**ğŸ“ ObservaÃ§Ã£o 1**: Aparentemente, um modelo polinomial de segundo grau pode oferecer um ajuste melhor a estes dados do que a regressÃ£o linear simples. Nosso objetivo serÃ¡ explorar esse modelo e, ao final, comparÃ¡-lo com o modelo linear.

**ğŸ“ ObservaÃ§Ã£o 2**: Como importamos os dados diretamente de um arquivo `.csv` para o objeto `df`, utilizamos a funÃ§Ã£o `scatterplot` da biblioteca [Seaborn](https://seaborn.pydata.org/) para plotar o grÃ¡fico de dispersÃ£o entre as variÃ¡veis $y$ e $x$.

## ğŸ§® Implementando o MMQ Polinomial - Passo a Passo

### Criando os Vetores Base

Para o modelo polinomial $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$, precisamos dos vetores:

$$\vec{f}_0 = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \quad \text{,} \quad \vec{f}_1 = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \quad \text{,} \quad \vec{f}_2 = \begin{bmatrix} x_1^2 \\ x_2^2 \\ \vdots \\ x_n^2 \end{bmatrix} \quad \text{e} \quad \vec{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$$

```{python}
# NÃºmero de observaÃ§Ãµes
n = len(df['x'])

# Vetor f0: vetor de 1's (para o intercepto Î²â‚€)
f0 = [1] * n

# Vetor f1: valores de x (para o coeficiente linear Î²â‚)
f1 = df['x'].copy()

# Vetor f2: valores de xÂ² (para o coeficiente quadrÃ¡tico Î²â‚‚)
f2 = np.array(df['x'])**2  # Eleva cada elemento de x ao quadrado
```


Visualizando os vetores $\vec{f}_0$, $\vec{f}_1$ e $\vec{f}_2$.
```{python}
print("Vetor f0 (intercepto):", f0)
print("Vetor f1 (termo linear):", f1)
print("Vetor f2 (termo quadrÃ¡tico):", f2)
```

### Construindo as Matrizes X e Y

Agora vamos montar as matrizes do sistema polinomial:

$$X = \begin{bmatrix} \vec{f}_0 & \vec{f}_1 & \vec{f}_2 \end{bmatrix} = \begin{bmatrix} 1 & x_1 & x_1^2 \\ 1 & x_2 & x_2^2 \\ \vdots & \vdots & \vdots \\ 1 & x_n & x_n^2 \end{bmatrix} \quad \text{e} \quad Y = \begin{bmatrix} \vec{y} \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$$

```{python}
# Matriz X: combinando f0, f1 e f2 em colunas
X = np.column_stack((f0, f1, f2))

# Matriz Y: transformando y em matriz com n linhas e 1 coluna
Y = np.array(df['y']).reshape(n, 1)
```

### Resolvendo o Sistema Normal

Calculamos os coeficientes usando a mesma fÃ³rmula: 

$$B = (X^T X)^{-1} X^T Y$$

```{python}
# Calculando X transposta vezes X
XTX = X.T @ X  # X.T Ã© a transposta de X
# Calculando X transposta vezes Y
XTY = X.T @ Y
# Calculando a matriz inversa (X^T X)^(-1)
XTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X
# Coeficientes de regressÃ£o
B = XTX_inv @ XTY
```

::: {.callout-note title="InterpretaÃ§Ã£o"}

- $\beta_0$ (intercepto): valor de y quando x = 0
- $\beta_1$ (coeficiente linear): relacionado Ã  taxa de variaÃ§Ã£o linear
- $\beta_2$ (coeficiente quadrÃ¡tico): relacionado Ã  curvatura da parÃ¡bola
  - Se $\beta_2 > 0$: parÃ¡bola com concavidade para cima
  - Se $\beta_2 < 0$: parÃ¡bola com concavidade para baixo

:::

### Obtendo os Valores Ajustados de y

Tendo obtido os coeficientes de regressÃ£o, os valores ajustados de y ($\hat{y}$) podem ser obtido pela multiplicaÃ§Ã£o matricial:

$$F = XB = \begin{bmatrix} 1 & x_1 & x^2_1 \\ 1 & x_2 & x^2_2 \\ \vdots & \vdots & \vdots \\ 1 & x_n & x^2_n \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}$$

**Obs.**: denominamos $F$ a matriz de valores ajustados de $y$.
```{python}
# Valores ajustados (preditos)
F = X @ B
```

### Avaliando a Qualidade do Ajuste

#### Calculando a Soma dos Quadrados dos ResÃ­duos ($SQ_{res}$)

$SQ_{res}$ pode ser obtida pela multiplicaÃ§Ã£o matricial:

$$SQ_{res} = \boldsymbol{e}^T \boldsymbol{e}$$

Em que $\boldsymbol{e}$ Ã© a matriz coluna dos **resÃ­duos** obtida pela diferenÃ§a entre os valores observados e ajustados de $y$:

$$\boldsymbol{e} = Y - F = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} - \begin{bmatrix} \hat{y}_1 \\ \hat{y}_2 \\ \vdots \\ \hat{y}_n \end{bmatrix} = \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{bmatrix}$$


```{python}
# ResÃ­duos: diferenÃ§a entre valores observados e ajustados
e = Y - F

# Soma dos Quadrados dos ResÃ­duos
SQres = (e.T @ e)[0, 0]
```

#### Calculando a Soma dos Quadrados Totais ($SQ_{tot}$)

$SQ_{tot}$ pode ser obtida pela multiplicaÃ§Ã£o matricial:

$$SQ_{tot} = \boldsymbol{D}^T \boldsymbol{D}$$

Em que $\boldsymbol{D}$ Ã© a matriz coluna dos **desvios da mÃ©dis** obtida pela diferenÃ§a entre os valores observados de $y$ e a mÃ©dia de $\overline{y}$:

$$\boldsymbol{D} = Y - \overline{Y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} - \begin{bmatrix} \overline{y} \\ \overline{y} \\ \vdots \\ \overline{y} \end{bmatrix} = \begin{bmatrix} d_1 \\ d_2 \\ \vdots \\ d_n \end{bmatrix}$$

```{python}
# Soma dos Quadrados Total
Y_medio = np.mean(Y)
D = Y - Y_medio
SQtot = (D.T @ D)[0, 0]
```

#### Calculando o coeficiente de determinaÃ§Ã£o $R^2$:

O $R^2$ Ã© dado pela expressÃ£o:

$$R^2 = 1 - \frac{SQ_{res}}{SQ_{tot}}$$

```{python}
# Coeficiente de DeterminaÃ§Ã£o RÂ²
R2 = 1 - (SQres / SQtot)
```

---

Visualizando os resultados:

```{python}
print("ğŸ“Š Medidas de Qualidade do Ajuste:")
print(f"Soma dos Quadrados dos ResÃ­duos (SQres): {SQres:.4f}")
print(f"Soma dos Quadrados Total (SQtot): {SQtot:.4f}")
print(f"Coeficiente de DeterminaÃ§Ã£o (RÂ²): {R2:.4f}")
print(f"Porcentagem da variaÃ§Ã£o explicada: {R2*100:.2f}%")
```

## ğŸ“Š Visualizando o Resultado Final

Vamos plotar os dados originais junto com a curva ajustada:

Criando uma linha `contÃ­nua` para $\hat{y}$
```{python}
# Criando pontos para desenhar a curva suave
x_curva = np.linspace(min(df['x']), max(df['x']), 100)
y_curva = B[0, 0] + B[1, 0] * x_curva + B[2, 0] * x_curva**2
```

```{python}
# Criando o grÃ¡fico final
plt.figure(figsize=(8, 6))

# Pontos observados
sns.scatterplot(data = df, x = 'x', y = 'y', 
                color = '#0072B2', s=120,
                label=f'Dados observados (n={n})')

# Valores ajustados
plt.scatter(df['x'], F[:,0], 
           color='#000000', marker='*', s=120, 
           label='Valores ajustados')

# Curva ajustada
plt.plot(x_curva, y_curva, 
         color='#D55E00', 
         label=fr'Curva ajustada: $\hat{{y}} = {B[0,0]:.3f} {B[1,0]:.3f}x + {B[2,0]:.3f}x^2$')

# ConfiguraÃ§Ãµes do grÃ¡fico
plt.title(f'RegressÃ£o Polinomial (2Âº grau) - MMQ\nRÂ² = {R2:.4f}', 
          fontsize=15, fontweight='bold')
plt.xlabel('VariÃ¡vel X', fontsize=12)
plt.ylabel('VariÃ¡vel Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend(fontsize=10)
plt.tight_layout()
plt.show()
```

## ğŸ¯ Resumo dos Resultados

```{python}
print("="*60)
print("         RESUMO DA REGRESSÃƒO POLINOMIAL")
print("="*60)
print(f"EquaÃ§Ã£o ajustada: y = {B[0,0]:.4f} {B[1,0]:.4f}x + {B[2,0]:.4f}xÂ²")
print(f"Coeficiente de determinaÃ§Ã£o (RÂ²): {R2:.4f}")
print(f"Porcentagem da variaÃ§Ã£o explicada: {R2*100:.2f}%")
print("="*60)
```

## ğŸ” ComparaÃ§Ã£o: Linear vs Polinomial

Vamos comparar o ajuste linear e polinomial para os mesmos dados:

```{python}
# Ajuste LINEAR para comparaÃ§Ã£o
X_linear = np.column_stack((f0, f1))  # Apenas f0 e f1
B_linear = np.linalg.inv(X_linear.T @ X_linear) @ (X_linear.T @ Y)

# RÂ² do modelo linear
F_linear = X_linear @ B_linear
residuos_linear = Y - F_linear
SQres_linear = (residuos_linear.T @ residuos_linear)[0, 0]
R2_linear = 1 - (SQres_linear / SQtot)

print("ğŸ“Š ComparaÃ§Ã£o dos Modelos:")
print("-" * 40)
print(f"Modelo Linear:     RÂ² = {R2_linear:.4f}")
print(f"Modelo Polinomial: RÂ² = {R2:.4f}")
print(f"Melhoria no RÂ²:    {R2 - R2_linear:.4f}")
```

GrÃ¡ficos de dispersÃ£o

```{python}
x_vals = df["x"].to_numpy()

y_linear = B_linear[0, 0] + B_linear[1, 0] * x_vals

# GrÃ¡fico comparativo
plt.figure(figsize=(8, 6))

# plt.subplot(1, 2, 1)
sns.scatterplot(data = df, x = 'x', y = 'y', s=100, color = '#0072B2', label='Dados observados')
plt.plot(x_vals, y_linear, color='#D55E00', label=f'Modelo Linear\nRÂ² = {R2_linear:.4f}')
plt.plot(x_curva, y_curva, color='#009E73', label=f'Modelo Polinomial\nRÂ² = {R2:.4f}')
plt.xlabel('X')
plt.ylabel('Y')
plt.grid(True, alpha=0.3)
plt.legend()

# plt.tight_layout()
plt.show()
```

## ğŸ§¾ Resumo do CÃ³digo (modelo polinomial)

1. InserÃ§Ã£o dos Dados
```{python}
df = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')
```

2. DefiniÃ§Ã£o das matrizes do sistema
```{python}
n = len(df['x'])
f0 = [1] * n
f1 = df['x'].copy()
f2 = np.array(df['x'])**2

X = np.column_stack((f0, f1, f2))
Y = np.array(df['y']).reshape(n, 1)
```

3. CÃ¡lculo dos coeficientes
```{python}
XTX = X.T @ X
XTY = X.T @ Y
XTX_inv = np.linalg.inv(XTX)
B = XTX_inv @ XTY
```

4. Qualidade do ajuste

```{python}
Y_ajustado = X @ B
e = Y - Y_ajustado
SQres = (e.T @ e)[0, 0]

Y_medio = np.mean(Y)
D = Y - Y_medio
SQtot = (D.T @ D)[0, 0]

R2 = 1 - (SQres / SQtot)
```


## ğŸš€ ExercÃ­cio PrÃ¡tico

Teste o cÃ³digo com novos dados:

```{python}
# Experimente com estes dados (padrÃ£o quadrÃ¡tico diferente):
df_novo = pd.DataFrame({
  'x_novo': [1, 2, 3, 4, 5, 6, 7],
  'y_novo': [30, 12, 18, 9, 7, 8, 6]
})

print(df_novo)

# QuestÃµes para investigar:
# 1. Qual Ã© o RÂ² do modelo polinomial para estes dados?
# 2. O coeficiente Î²â‚‚ Ã© positivo ou negativo? O que isso significa?
# 3. Compare com o modelo linear - qual Ã© a diferenÃ§a no RÂ²?

# Implemente todo o processo do MMQ polinomial com os novos dados
# Dica: vocÃª pode copiar e adaptar o cÃ³digo acima!
```

## ğŸ’¡ Conceitos Importantes Revisados

1. **RegressÃ£o Polinomial**: ExtensÃ£o da regressÃ£o linear para relaÃ§Ãµes curvas
2. **Matriz de Design**: Agora com trÃªs colunas: $[1, x, x^2]$
3. **InterpretaÃ§Ã£o dos Coeficientes**: Cada coeficiente tem significado especÃ­fico
4. **ComparaÃ§Ã£o de Modelos**: Uso do $R^2$ para avaliar qual modelo Ã© melhor

## ğŸ”— PrÃ³ximos Passos

- Experimente com polinÃ´mios de grau maior ($x^3$, $x^4$, etc.)
- Investigue o conceito de **overfitting** com graus muito altos
