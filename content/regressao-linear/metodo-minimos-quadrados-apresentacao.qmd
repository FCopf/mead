---
title: "Método dos Mínimos Quadrados na Regressão Linear Simples"
subtitle: "Representação Vetorial e Matricial"
institute: "9833 - BASES DA MATEMÁTICA E ESTATÍSTICA PARA CIÊNCIAS DO MAR"
author: 	"Fabio Cop Ferreira e William Remo Pedroso Conti"
image: "images/metodo-minimos-quadrados-apresentacao.png"
description: "Método dos mínimos quadrados na Regressão linear simples por meio da representação vetorial e matricial."
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: images/metodo-minimos-quadrados-apresentacao.png 
    footer: |
      BICT Mar - Unifesp · <a href="/ecologia-numerica.html" target="_blank">Ecologia Numérica</a>
    theme: [default, custom.scss]
title-slide-attributes:
   data-background: "#d6e0e4" #"linear-gradient(135deg, #d0dee4, #b8c0ce, #adc8f1)"
   # data-background-image: images/watershed-graph.png
   # data-background-size: cover
   # background-repeat: no-repeat
   # data-background-opacity: "0.45"
    
---

## Conteúdo da Aula

::: {.content-box}
1. Introdução à Regressão Linear Simples
2. Definição dos Resíduos
3. Método dos Mínimos Quadrados
4. Representação Vetorial dos Resíduos
5. Geometria da Solução de Mínimos Quadrados
6. Solução Matricial do Método dos Mínimos Quadrados
7. Valores preditos
8. Soma dos quadrados dos resíduos
9. Soma dos quadrados dos totais
10. Coeficiente de determinação
:::

## Introdução à Regressão Linear Simples

A regressão linear simples é um método para modelar a relação entre uma variável dependente $y$ e uma variável independente $x$. A equação da reta ajustada é dada por:

$$ \hat{y} = \beta_0 + \beta_1 x $$

::: {style="font-size: .6em"}

:::: columns
::: {.column width="30%"}

| Observação | $x_i$ | $y_i$ |
|------------|---------|---------|
| $1$      | $x_1$ | $y_1$ |
| $2$      | $x_2$ | $y_2$ |
| $3$      | $x_3$ | $y_3$ |
| $\vdots$ | $\vdots$ | $\vdots$ |
| $n$      | $x_n$ | $y_n$ |


:::

::: {.column width="70%"}

![](images/diagrama-regressao.png){style="background-color:#ffffff; width:600px"}

:::
::::

:::

## Definição dos Resíduos

Na figura abaixo, os resíduos $e_i$ representam as diferenças entre os valores observados $y_i$ e os valores ajustados $\hat{y}_i$ pela reta de regressão:

$$ e_i = y_i - (\beta_0 + \beta_1 x_i) $$

Portando na regressão linear, assume-se que o valor observado em $y_i$ é dado por:

$$ y_i = \beta_0 + \beta_1 x_i + e_i$$

::: {.callout-tip}



:::: columns

::: {.column width="50%"}

Acesse o link [Regresão linear Geogebra](https://www.geogebra.org/calculator/d8a9x9j3){target="_blank"}

::: {style="font-size: .6em"}

<div style="text-align: center;">

| Observação | $x_i$ | $y_i$ |
|------------|---------|---------|
| $1$      | $x_1$ | $y_1$ |
| $2$      | $x_2$ | $y_2$ |
| $3$      | $x_3$ | $y_3$ |
| $\vdots$ | $\vdots$ | $\vdots$ |
| $n$      | $x_n$ | $y_n$ |

</div>

:::

:::

::: {.column width="50%"}

<div style="text-align: center;">

![](images/diagrama-regressao.png){style="background-color:#ffffff; width:70%"}

</div>
:::
::::



:::

## Método dos Mínimos Quadrados

O Método dos Mínimos Quadrados busca minimizar a soma dos quadrados dos resíduos:

$$ SQ_{res} = \sum_{i=1}^{n} e_i^2 = e_1^2 + e_2^2 + \cdots + e_n^2 $$

Que pode ser representada como:

$$
\begin{cases}
e_1 = y_1 - (\beta_0 + \beta_1 x_1) \\
e_2 = y_2 - (\beta_0 + \beta_1 x_2) \\
\vdots \\
e_n = y_n - (\beta_0 + \beta_1 x_n) \\
\end{cases}
$$

## Representação Vetorial dos Resíduos

Podemos portanto representar os resíduos como vetor em que o vetor $\vec{e}$ é igual ao vetor $y$ menos uma **combinação linear** dos vetores $\vec{f}_0$ e $\vec{f}_0$ com constantes $\beta_0$ e $\beta_1$.

</br>

$$ \vec{e} = \vec{y} - (\beta_0 \vec{f}_0 + \beta_1 \vec{f}_1) $$

</br>

::: {style="font-size: .7em"}

$$
\left[ \begin{array}{c}
e_1 \\
e_2 \\
\vdots \\
e_n \\
\end{array} \right]
= 
\left[ \begin{array}{c}
y_1 - (\beta_0 + \beta_1 x_1) \\
y_2 - (\beta_0 + \beta_1 x_2) \\
\vdots \\
y_n - (\beta_0 + \beta_1 x_n) \\
\end{array} \right] 
=
\left[ \begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{array} \right]
-
\left(
\beta_0
\left[ \begin{array}{c}
1 \\
1 \\
\vdots \\
1 \\
\end{array} \right]
+
\beta_1
\left[ \begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{array} \right]
\right)
$$


Onde:

$$\vec{e} =
\left[ \begin{array}{c}
e_1 \\
e_2 \\
\vdots \\
e_n \\
\end{array} \right];
\vec{y} =
\left[ \begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n \\
\end{array} \right];
\vec{f}_0 =
\left[ \begin{array}{c}
1 \\
1 \\
\vdots \\
1 \\
\end{array} \right];
\vec{f}_1 =
\left[ \begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{array} \right]$$

:::

## Geometria da Solução de Mínimos Quadrados

A Soma dos quadrados dos resíduos ($SQ_{res}$) pode ser obtida pela **norma ao quadrado** do vetor $\vec{e}$:

$$SQ_{res} = \Vert\vec{e}\Vert^{2}=\vec{e}\cdot\vec{e}=e_{1}^{2}+e_{2}^{2}+\cdots+e_{n}^{2}$$

</br>

::: {.callout-tip title='Representação da Solução do MMQ no GeoGebra'}

O Método dos Mínimos Quadrados determina $\beta_0$ e $\beta_1$ de modo a *minimizar o comprimento* (a norma) do vetor $\vec{e}$ que pode ser obtida impondo que o vetor $\vec{e}$ seja ortogonal aos vetores $\vec{f_0}$ e $\vec{f_1}$, ou seja:

$$ \vec{f_0} \cdot \vec{e} = 0 $$
$$ \vec{f_1} \cdot \vec{e} = 0 $$

[Link para solução vetorial do MMQ](https://www.geogebra.org/calculator/eyk8ammw){target="_blank"}

:::

## Geometria da Solução de Mínimos Quadrados

$$
\left\{\begin{array} {c}
\vec{f_0} \cdot \vec{e} = 0 \Leftrightarrow \vec{f_0}\cdot(\vec{y}-\beta_0\vec{f_0}-\beta_1\vec{f_1})=0\\
\vec{f_1} \cdot \vec{e} = 0 \Leftrightarrow \vec{f_1}\cdot(\vec{y}-\beta_0\vec{f_0}-\beta_1\vec{f_1})=0
\end{array} \right.
$$
que é equivalente a:
$$
\left\{\begin{array} {c}
\beta_0\vec{f_0}\cdot\vec{f_0}+\beta_1\vec{f_0}\cdot\vec{f_1}=\vec{f_0}\cdot\vec{y}\\
\beta_0\vec{f_1}\cdot\vec{f_0}+\beta_1\vec{f_1}\cdot\vec{f_1}=\vec{f_1}\cdot\vec{y}
\end{array} \right.
,
$$
que ainda pode ser escrito na forma matricial:
$$
\left[ \begin{array}{cc}
\vec{f_0}\cdot\vec{f_0} & \vec{f_0}\cdot\vec{f_1}\\
\vec{f_1}\cdot\vec{f_0} & \vec{f_1}\cdot\vec{f_1}
\end{array} \right]
\left[ \begin{array}{c}
\beta_0\\
\beta_1
\end{array} \right]
=
\left[ \begin{array}{c}
\vec{f_0}\cdot\vec{y}\\
\vec{f_1}\cdot\vec{y}
\end{array} \right]
$$

## Solução Matricial do Método dos Mínimos Quadrados

A combinação linear:

$$
\left[ \begin{array}{cc}
\vec{f_0}\cdot\vec{f_0} & \vec{f_0}\cdot\vec{f_1}\\
\vec{f_1}\cdot\vec{f_0} & \vec{f_1}\cdot\vec{f_1}
\end{array} \right]
\left[ \begin{array}{c}
\beta_0\\
\beta_1
\end{array} \right]
=
\left[ \begin{array}{c}
\vec{f_0}\cdot\vec{y}\\
\vec{f_1}\cdot\vec{y}
\end{array} \right]
$$

por ser expressa pelas matrizes:

$$X = \left[ \begin{array}{ccc}
\vec{f_0} & \vdots & \vec{f_1}
\end{array} \right] = 
\left[ \begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n \\
\end{array} \right]; 
Y = \left[ \begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n \\
\end{array} \right];
B = \left[ \begin{array}{c}
\beta_0 \\
\beta_1 \\
\end{array} \right]
$$

E finalmente:

$$B = (X^{T} X)^{-1}(X^{T}Y)$$

## Calculando os valores preditos ($\hat{y}$)

Definimos $\mathbf{F}$ como a matriz coluna que contém os valores **preditos** de $y$ (denominados $\hat{y}$), isto é, aquela que contém os pontos em $y$ que se sobrepõem à reta da regressão linear. Podemos obter $\mathbf{F}$ por meio da operação matricial abaixo:

<hr>
$$\mathbf{F} = \mathbf{X}\mathbf{B}$$
<hr>

$$\mathbf{F} = \left[ \begin{array}{c}
\hat{y}_1 \\
\hat{y}_2 \\
\vdots & \vdots \\
\hat{y}_n \\
\end{array} \right]; \mathbf{X} = \left[ \begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n \\
\end{array} \right]; \mathbf{B} = \left[ \begin{array}{c}
\beta_0 \\
\beta_1 \\
\end{array} \right]
$$

## Vetor de resíduos ($e$)

Finalmente, o vetor de resíduos é obtido por:

<hr>
$$e = \mathbf{Y} - \mathbf{F}$$
<hr>

Agora temos todos os componentes da regressão linear estabelecida inicialmente:

$$ \hat{y_i} = \beta_0 + \beta_1 x_i $$

e 

$$ y_i = \hat{y_i} + e_i $$

## Soma dos quadrados dos resíduos ($SQ_{res}$)

A Soma dos quadrados dos resíduos foi definida pela expressão abaixo:

$$SQ_{res} = \Vert\vec{e}\Vert^{2}=\vec{e}\cdot\vec{e}=e_{1}^{2}+e_{2}^{2}+\cdots+e_{n}^{2}$$

Considerando $\vec{e}$ como a *matriz coluna* $\mathbf{e}$:

$$\mathbf{e} = \left[ \begin{array}{c}
e_1 \\
e_2 \\
\vdots \\
e_n \\
\end{array} \right]
$$

Podemos fazer:

<hr>

$$SQ_{res} = \mathbf{e}^\top \mathbf{e}$$

<hr>

## Soma dos quadrados totais ($SQ_{tot}$)

$SQ_{tot}$ pode ser definido como:

<hr>

$$SQ_{tot} = \sum_{i}^{n}{(y_i - \overline{y})^{2}} = (y_1 - \overline{y})^{2} + (y_2 - \overline{y})^{2} + \cdots + (y_n - \overline{y})^{2}$$

em que $\overline{y}$ é a **média aritmética** de $y$

<hr>

::: {.columns}
::: {.column width="50%"}
Podemos definir a matrix coluna $\mathbf{D}$

$$\mathbf{D} = \left[ \begin{array}{c}
(y_1 - \overline{y})^{2} \\
(y_2 - \overline{y})^{2} \\
\vdots\\
(y_n - \overline{y})^{2} \\
\end{array} \right]
$$

:::

::: {.column width="50%"}
E obter $SQ_{tot}$ por:

$$SQ_{tot} = \mathbf{D}^\top \mathbf{D}$$

:::
:::

## Coeficiente de determinação ($R^2$)

A qualidade do ajuste pode ser determinada pelo **coeficiente de determinação** ($R^2$), um índice que varia entre 0 e 1.

<hr>
$$R^2 = 1 - \frac{SQ_{res}}{SQ_{tot}}$$
<fr>

## Coeficiente de determinação ($R^2$)

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

def simula_regressao_plot(x, beta0, beta1, error_sd, random_state=None):
    if random_state is not None:
        np.random.seed(random_state)

    # Simulando os dados
    y = np.random.normal(loc=beta0 + beta1 * x, scale=error_sd)
    
    # Ajustando o modelo de regressão
    modelo = LinearRegression()
    modelo.fit(x.reshape(-1, 1), y)
    y_pred = modelo.predict(x.reshape(-1, 1))
    r = modelo.score(x.reshape(-1, 1), y)

    # Criando figura e eixo — não será exibida automaticamente
    fig, ax = plt.subplots()
    ax.scatter(x, y, color='blue', label='Dados', s=80)
    ax.plot(x, y_pred, color='red', linewidth=2, label='Reta de Regressão')
    ax.set_title(rf'$\beta_1 = {beta1}$, $\sigma = {error_sd}$' + '\n' + rf'$R^2 = {r:.2f}$')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.grid(True, linestyle='--', alpha=0.7)
    ax.legend()
    plt.close(fig)

    return r, y_pred, y, fig 


```

```{python}
x = np.arange(0, 11)
iseed = 2
# Gerando as 4 figuras individualmente
r1, y_pred1, y1, fig1 = simula_regressao_plot(x, 12, 2, 0, random_state=iseed)
r2, y_pred2, y2, fig2 = simula_regressao_plot(x, 12, 2, 3, random_state=iseed)
r3, y_pred3, y3, fig3 = simula_regressao_plot(x, 12, 0, 0, random_state=iseed)
r4, y_pred4, y4, fig4 = simula_regressao_plot(x, 12, 0, 30, random_state=iseed)

# Composição em um grid 2x2 com subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 6.5))

for ax, y, y_pred, r, beta1, error_sd in zip(
    axes.ravel(),
    [y1, y2, y3, y4],
    [y_pred1, y_pred2, y_pred3, y_pred4],
    [r1, r2, r3, r4],
    [2, 2, 0, 0],
    [0, 30, 0, 30]
):
    ax.scatter(x, y, color='blue', label='Dados', s=80)
    ax.plot(x, y_pred, color='red', linewidth=2, label='Reta de Regressão')
    ax.set_title(rf'$\beta_1 = {beta1}$, $\sigma = {error_sd}$' + '\n' + rf'$R^2 = {r:.2f}$')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.grid(True, linestyle='--', alpha=0.7)
    ax.legend()

plt.tight_layout()
plt.show()

```

## Método dos mínimos quadrados: Resumo dos passos {.smaller}

::: {.columns}
::: {.column width="50%"}

::: {.callout-tip title="Resolução do MMQ"}

1. Definição das matrizes do sistema

$$X = \left[ \begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n \\
\end{array} \right]; 
Y = \left[ \begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n \\
\end{array} \right];
B = \left[ \begin{array}{c}
\beta_0 \\
\beta_1 \\
\end{array} \right]
$$

2. Cálculo dos coeficientes

$$B = (X^{T} X)^{-1}(X^{T}Y)$$

3. Valores preditos

$$\mathbf{F} = \mathbf{X}\mathbf{B}$$


4. Matriz coluna de Resíduos

$$\mathbf{e} = \mathbf{Y} - \mathbf{F}$$

:::

:::
::: {.column width="50%"}

::: {.callout-tip title="Qualidade do ajuste"}

5. Soma dos quadrados dos resíduos

$$SQ_{res} = \mathbf{e}^\top \mathbf{e}$$


6. Soma dos quadrados totais

$$SQ_{tot} = \mathbf{D}^\top \mathbf{D}$$


7. Coeficiente de determinação

$$R^2 = 1 - \frac{SQ_{res}}{SQ_{tot}}$$

:::

:::
:::
